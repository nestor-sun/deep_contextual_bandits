from multiprocessing import Pool
import sys
sys.path.append('../model')
import torch
import torch.nn as nn
from torch.autograd import Variable
import numpy as np
import json
# from tqdm import tqdm


class Experiment:
    def __init__(self, num_of_iters, model_path, learning_rate, epsilon, c, perturbation_var):
        self.user_path = '../../data/ads_16/ADS16_Benchmark_part2/user/'
        # self.max_reward_dict = json.load(open('D:/study/gumd/UMD_course/machine learning guarantees and analyses/deep_contextual_bandits/data/ads_16/ADS16_Benchmark_part2/max_rating_for_each_user.json', 'r'))
        # predictive model initialization
        self.path = model_path
        self.model_ts = torch.load(model_path + 'model.dat')
        self.model_ucb = torch.load(model_path + 'model.dat')
        self.model_epsilon = torch.load(model_path + 'model.dat')
        self.model_ts.train()
        self.model_ucb.train()
        self.model_epsilon.train()

        self.criterion = nn.MSELoss().cuda()
        self.optimizer_ts = torch.optim.Adam(self.model_ts.parameters(), lr=learning_rate)
        self.optimizer_ucb = torch.optim.Adam(self.model_ucb.parameters(), lr=learning_rate)
        self.optimizer_epsilon = torch.optim.Adam(self.model_epsilon.parameters(), lr=learning_rate)

        # contextual bandits initialization
        self.num_of_iters = num_of_iters
        self.perturbation_var = perturbation_var  # var used to change actual user rating
        self.c = c  # ucb exploration parameter
        self.epsilon = epsilon
        self.num_of_arms = 10

        self.cat_img_id_list = [str(category) + '_' + str(image) for category in range(11, 21) for image in range(1, 16)]

        self.k_reward = np.zeros(len(self.cat_img_id_list))  # for epsilon greedy
        self.k_n_ucb = np.ones(len(self.cat_img_id_list))  # for ucb
        self.k_n_epsilon = np.ones(len(self.cat_img_id_list))  # for epsilon
        self.k_reward_list = [[1.0] for i in range(len(self.cat_img_id_list))]  # for ts
        self.n = 1  # Step count

        # self.regret_list_ts = []
        # self.regret_list_ucb = []
        # self.regret_list_epsilon = []

        self.reward_list_ts = []
        self.reward_list_ucb = []
        self.reward_list_epsilon = []
        self.reward_list_random = []
        self.reward_list_optimal = []

    def save_model(self, output_path):
        torch.save(self.model_ts, output_path + 'model_ts.dat')
        torch.save(self.model_ucb, output_path + 'model_ucb.dat')
        torch.save(self.model_epsilon, output_path + 'model_epsilon.dat')

    def pull(self, user_pca_feat):
        ts_choices = torch.zeros(150)
        ucb_choices = torch.zeros(150)
        epsilon_choices = torch.zeros(150)

        row_num = 0
        for id, feat in user_pca_feat.items():
            predicted_rating_ts = self.model_ts(Variable(torch.tensor(feat)).cuda())
            predicted_rating_ucb = self.model_ucb(Variable(torch.tensor(feat)).cuda())
            predicted_rating_epsilon = self.model_epsilon(Variable(torch.tensor(feat)).cuda())

            ts_value = torch.normal(predicted_rating_ts, torch.var(torch.tensor(self.k_reward_list[self.cat_img_id_list.index(id)])))
            ucb_value = predicted_rating_ucb + self.c * torch.sqrt(torch.tensor((np.log(self.n)) / self.k_n_ucb[self.cat_img_id_list.index(id)]))

            ts_choices[row_num] = ts_value
            ucb_choices[row_num] = ucb_value
            epsilon_choices[row_num] = predicted_rating_epsilon
            row_num += 1
        if np.random.uniform() <= self.epsilon:  # explore
            epsilon_choice = np.random.randint(0, len(self.cat_img_id_list)-1)
            # predicted_rating_epsilon = self.model_epsilon(Variable(torch.tensor(user_pca_feat[self.cat_img_id_list[epsilon_choice]])).cuda())

        else:
            epsilon_choice = torch.argmax(epsilon_choices)
            # predicted_rating_epsilon = torch.max(epsilon_choices)
        # print('---', len(ucb_choices),  int(np.argmax(ucb_choices)), ucb_choices)

        random_choice = np.random.randint(0, len(self.cat_img_id_list)-1)

        return int(torch.argmax(ts_choices)), int(torch.argmax(ucb_choices)), int(epsilon_choice), int(random_choice)

    def run(self):
        for i in range(self.num_of_iters):
            user = np.random.randint(61, 121)
            # print(user)
            # user_max_reward = self.max_reward_dict[str(user)]
            if user <= 9:
                user_id = 'U000' + str(user)
            elif user >= 100:
                user_id = 'U0' + str(user)
            else:
                user_id = 'U00' + str(user)

            user_pca_feat = json.load(open(self.user_path + user_id + '_pca_feat.json', 'r'))
            user_rating = json.load(open(self.user_path + user_id + '_rating.json', 'r'))

            ts_choice, ucb_choice, epsilon_choice, random_choice = self.pull(user_pca_feat)

            # map from list position to cat_img id
            id_for_ts = self.cat_img_id_list[ts_choice]
            id_for_ucb = self.cat_img_id_list[ucb_choice]
            id_for_epsilon = self.cat_img_id_list[epsilon_choice]
            id_for_random = self.cat_img_id_list[random_choice]
            id_for_optimal = np.argmax(self.cat_img_id_list)

            # actual user rating with perturbation
            if self.perturbation_var > 0:
                rating_for_ts       = np.random.normal(user_rating[id_for_ts], self.perturbation_var)
                rating_for_ucb      = np.random.normal(user_rating[id_for_ucb], self.perturbation_var)
                rating_for_epsilon  = np.random.normal(user_rating[id_for_epsilon], self.perturbation_var)
                rating_for_random   = np.random.normal(user_rating[id_for_random], self.perturbation_var)
            else:
                rating_for_ts       = float(user_rating[id_for_ts])
                rating_for_ucb      = float(user_rating[id_for_ucb])
                rating_for_epsilon  = float(user_rating[id_for_epsilon])
                rating_for_random   = float(user_rating[id_for_random])
            rating_for_optimal   = float(user_rating[id_for_optimal])

            self.n += 1
            self.k_n_ucb[ucb_choice] += 1
            self.k_reward_list[ts_choice].append(rating_for_ts)
            self.k_reward[epsilon_choice] = self.k_reward[epsilon_choice] + \
                                (rating_for_epsilon - self.k_reward[epsilon_choice]) / self.k_n_epsilon[epsilon_choice]

            # import pdb
            # pdb.set_trace()

            self.optimizer_ts.zero_grad()
            self.optimizer_ucb.zero_grad()
            self.optimizer_epsilon.zero_grad()

            predicted_rating_ts = self.model_ts(Variable(torch.tensor(user_pca_feat[id_for_ts])).cuda())
            predicted_rating_ucb = self.model_ucb(Variable(torch.tensor(user_pca_feat[id_for_ucb])).cuda())
            predicted_rating_epsilon = self.model_epsilon(Variable(torch.tensor(user_pca_feat[id_for_epsilon])).cuda())

            loss_ts = self.criterion(predicted_rating_ts, Variable(torch.tensor(rating_for_ts)).cuda())
            loss_ucb = self.criterion(predicted_rating_ucb, Variable(torch.tensor(rating_for_ucb)).cuda())
            loss_epsilon = self.criterion(predicted_rating_epsilon, Variable(torch.tensor(rating_for_epsilon)).cuda())

            # print(loss_ts, loss_ucb, loss_epsilon)

            loss_ts.backward()
            loss_ucb.backward()
            loss_epsilon.backward()

            self.optimizer_ts.step()
            self.optimizer_ucb.step()
            self.optimizer_epsilon.step()

            # regret_ts = user_max_reward - rating_for_ts
            # regret_ucb = user_max_reward - rating_for_ucb
            # regret_epsilon = user_max_reward - rating_for_epsilon

            self.reward_list_ts.append(rating_for_ts)
            self.reward_list_ucb.append(rating_for_ucb)
            self.reward_list_epsilon.append(rating_for_epsilon)
            self.reward_list_random.append(rating_for_random)
            self.reward_list_optimal.append(rating_for_optimal)
            
        return self.reward_list_ts, self.reward_list_ucb, self.reward_list_epsilon, self.reward_list_random, self.reward_list_optimal

def Experiment_wrapper(args):
    i=0
    num_of_iters = args[i]
    i+=1
    model_path = args[i]
    i+=1
    learning_rate = args[i]
    i+=1
    epsilon  = args[i]
    i+=1
    c = args[i]
    i+=1
    reward_perturbation_var = args[i]

    ex = Experiment(num_of_iters, model_path, learning_rate, epsilon, c, reward_perturbation_var)
    ts_reward, ucb_reward, epsilon_reward, random_reward, optmal_reward = ex.run()
    return [ts_reward, ucb_reward, epsilon_reward, random_reward, optmal_reward]

num_of_iters = 100
model_path = '../model/'
learning_rate = 0.0001
epsilon = 0.1
c = 1
reward_perturbation_var = 0
num_of_episodes = 30
cpu_threads = 15

for i in range(len(sys.argv)):
    if 'epsilon' == sys.argv[i]:
        epsilon = float(sys.argv[i+1])
    if 'reward_perturbation_var' == sys.argv[i]:
        reward_perturbation_var = float(sys.argv[i+1])
    if 'num_of_episodes' == sys.argv[i]:
        num_of_episodes = int(sys.argv[i+1])
    if 'num_of_iters' == sys.argv[i]:
        num_of_iters = int(sys.argv[i+1])
    if 'cpu_threads' == sys.argv[i]:
        cpu_threads = int(sys.argv[i+1])


parallel_args = []
for j in range(num_of_episodes):
    parallel_args.append([num_of_iters, model_path, learning_rate, epsilon, c, reward_perturbation_var])

# run_results = list(map(Experiment_wrapper, iter(parallel_args)))
with Pool(cpu_threads) as p:
    run_results = list(p.map(Experiment_wrapper, iter(parallel_args)))

ts_reward_list = []
ucb_reward_list = []
epsilon_reward_list = []
random_reward_list = []
optimal_reward_list = []
for i in range(num_of_episodes):

    # ex = Experiment(num_of_iters, model_path, learning_rate, epsilon, c, reward_perturbation_var)
    # ts_reward, ucb_reward, epsilon_reward, random_reward = ex.run()

    j=0
    ts_reward = run_results[i][j]
    j+=1
    ucb_reward = run_results[i][j]
    j+=1
    epsilon_reward = run_results[i][j]
    j+=1
    random_reward = run_results[i][j]
    j+=1
    optmal_reward = run_results[i][j]

    ts_reward_list.append(ts_reward)
    ucb_reward_list.append(ucb_reward)
    epsilon_reward_list.append(epsilon_reward)
    random_reward_list.append(random_reward)
    optimal_reward_list.append(optmal_reward)

output_path = '../../data/ads_16/experiment_output/'
json.dump(ts_reward_list, open(output_path + 'ts_reward_list_reward_var_'+str(reward_perturbation_var)
    +'_episodes_'+str(num_of_episodes)+'_epsilon_'+str(epsilon)+'_num_of_iters_'+str(num_of_iters)+'.json', 'w'))
json.dump(ucb_reward_list, open(output_path + 'ucb_reward_list_reward_var_'+str(reward_perturbation_var)
    +'_episodes_'+str(num_of_episodes)+'_epsilon_'+str(epsilon)+'_num_of_iters_'+str(num_of_iters)+'.json', 'w'))
json.dump(epsilon_reward_list, open(output_path + 'epsilon_reward_list_reward_var_'+str(reward_perturbation_var)
    +'_episodes_'+str(num_of_episodes)+'_epsilon_'+str(epsilon)+'_num_of_iters_'+str(num_of_iters)+'.json', 'w'))
json.dump(random_reward_list, open(output_path + 'random_reward_list_reward_var_'+str(reward_perturbation_var)
    +'_episodes_'+str(num_of_episodes)+'_epsilon_'+str(epsilon)+'_num_of_iters_'+str(num_of_iters)+'.json', 'w'))
json.dump(optimal_reward_list, open(output_path + 'optimal_reward_list_reward_var_'+str(reward_perturbation_var)
    +'_episodes_'+str(num_of_episodes)+'_epsilon_'+str(epsilon)+'_num_of_iters_'+str(num_of_iters)+'.json', 'w'))

