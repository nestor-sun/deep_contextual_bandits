{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "# Top k-Ranking \n",
    "\n",
    "class MAB:\n",
    "    def __init__(self, arms_len, max_k, mode):\n",
    "        self.arms_len = arms_len\n",
    "        self.max_k = max_k\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.arms_reward = np.zeros(self.arms_len)\n",
    "        self.arms_expectation = .00001*np.random.random_sample((self.arms_len,)) \n",
    "        self.arms_expectation_counts = np.zeros(self.arms_len)\n",
    "        \n",
    "    def choose_arms(self):\n",
    "        if self.mode == 'random':\n",
    "            return np.random.permutation(self.arms_len)[0:self.max_k]\n",
    "        elif self.mode == 'epsilon_greedy1':\n",
    "            if np.random.random_sample() < .1:\n",
    "                return np.random.permutation(self.arms_len)[0:self.max_k]\n",
    "            else:\n",
    "                arms_expectation_sorted_index = np.flip(np.argsort(self.arms_expectation))\n",
    "                return arms_expectation_sorted_index[0:self.max_k]\n",
    "        elif self.mode == 'epsilon_greedy2':\n",
    "            if np.random.random_sample() < .5:\n",
    "                return np.random.permutation(self.arms_len)[0:self.max_k]\n",
    "            else:\n",
    "                arms_expectation_sorted_index = np.flip(np.argsort(self.arms_expectation))\n",
    "                return arms_expectation_sorted_index[0:self.max_k]\n",
    "        elif self.mode == 'epsilon_greedy3':\n",
    "            if np.random.random_sample() < .9:\n",
    "                return np.random.permutation(self.arms_len)[0:self.max_k]\n",
    "            else:\n",
    "                arms_expectation_sorted_index = np.flip(np.argsort(self.arms_expectation))\n",
    "                return arms_expectation_sorted_index[0:self.max_k]\n",
    "        elif self.mode == 'max_expectation':\n",
    "            arms_expectation_sorted_index = np.flip(np.argsort(self.arms_expectation))\n",
    "            return arms_expectation_sorted_index[0:self.max_k]\n",
    "        elif self.mode == 'optimal':\n",
    "            if self.max_k == 1:\n",
    "                if 5 < self.arms_len:\n",
    "                    return np.array([5])\n",
    "                else:\n",
    "                    return self.arms_len-1\n",
    "            elif self.max_k == 2:\n",
    "                if 6 < self.arms_len:\n",
    "                    return np.array([5, 6])\n",
    "                else:\n",
    "                    return np.array(range(self.arms_len-2, self.arms_len))\n",
    "            elif self.max_k == 3:   \n",
    "                if 6 < self.arms_len:\n",
    "                    return np.array([4, 5, 6])\n",
    "                else:\n",
    "                    return np.array(range(self.arms_len-3, self.arms_len))\n",
    "            elif self.max_k == 4:\n",
    "                if 7 < self.arms_len:\n",
    "                    return np.array([4, 5, 6, 7])\n",
    "                else:\n",
    "                    return np.array(range(self.arms_len-4, self.arms_len))\n",
    "            elif self.max_k == 5:\n",
    "                if 7 < self.arms_len:\n",
    "                    return np.array([3, 4, 5, 6, 7])\n",
    "                else:\n",
    "                    return np.array(range(self.arms_len-5, self.arms_len))\n",
    "            elif self.max_k == 6:\n",
    "                if 8 < self.arms_len:\n",
    "                    return np.array([3, 4, 5, 6, 7, 8])\n",
    "                else:\n",
    "                    return np.array(range(self.arms_len-6, self.arms_len))\n",
    "        elif 'ucb' in self.mode:\n",
    "            arms_std = np.zeros(self.arms_len)\n",
    "            for i in range(self.arms_len):\n",
    "                for j in range(len(self.rewards)):\n",
    "                    if i in self.rewards[j][0]:\n",
    "                        arms_std[i] = arms_std[i] + np.power(self.rewards[j][1] - self.arms_expectation[i], 2)\n",
    "            arms_std = np.power(arms_std,1/2)\n",
    "\n",
    "            arms_ucb = np.zeros(self.arms_len)\n",
    "            for i in range(self.arms_len):\n",
    "                if self.arms_expectation_counts[i]>0:\n",
    "                    if self.mode == 'ucb':\n",
    "                        arms_ucb[i] = self.arms_expectation[i] + arms_std[i]/self.arms_expectation_counts[i]\n",
    "                    if self.mode == 'ucb2':\n",
    "                        arms_ucb[i] = self.arms_expectation[i] + arms_std[i]*np.maximum(0, 30-self.arms_expectation_counts[i])\n",
    "                else:\n",
    "                    arms_ucb[i] = self.arms_expectation[i] + arms_std[i]\n",
    "                \n",
    "            arms_ucb_sorted_index = np.flip(np.argsort(arms_ucb))\n",
    "            return arms_ucb_sorted_index[0:self.max_k]\n",
    "        else:\n",
    "            print('choose_arms unknown mode error')\n",
    "    \n",
    "    def set_reward(self, arms, reward):\n",
    "        self.rewards.append((arms, reward))\n",
    "        for arm in arms:\n",
    "            arm = int(arm)\n",
    "            self.arms_reward[arm] += reward\n",
    "            self.arms_expectation_counts[arm] += 1\n",
    "            self.arms_expectation[arm] = self.arms_reward[arm]/self.arms_expectation_counts[arm]\n",
    "\n",
    "def test_model_reward(MAB,reward_noise,epochs):\n",
    "\n",
    "    reward_vector = np.zeros(epochs)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        arms = MAB.choose_arms()\n",
    "        error = np.zeros(arms.shape[0])\n",
    "        for j in range(arms.shape[0]):\n",
    "            arm = arms[j]\n",
    "            error[j] = np.absolute(arm - 5)\n",
    "        total_error = np.sum(error)\n",
    "        # if MAB.max_k == 1:\n",
    "        #     total_error = np.sum(error)\n",
    "        # elif MAB.max_k == 2:\n",
    "        #     total_error = np.sum(error)-1\n",
    "        # elif MAB.max_k == 3:\n",
    "        #     total_error = np.sum(error)-2\n",
    "        # elif MAB.max_k == 4:\n",
    "        #     total_error = np.sum(error)-4\n",
    "        # elif MAB.max_k == 5:\n",
    "        #     total_error = np.sum(error)-6\n",
    "        # elif MAB.max_k == 6:\n",
    "        #     total_error = np.sum(error)-9\n",
    "        total_error = total_error + np.abs(np.random.randn()*reward_noise)\n",
    "        reward = -total_error\n",
    "        MAB.set_reward(arms, reward)\n",
    "        if i>0:\n",
    "            reward_vector[i] = reward + reward_vector[i-1]\n",
    "        else:\n",
    "            reward_vector[i] = reward\n",
    "\n",
    "    for i in range(epochs):\n",
    "        reward_vector[i] = reward_vector[i]/(i+1)\n",
    "\n",
    "    return reward_vector\n",
    "\n",
    "def test_model_reward_wrapper(args):\n",
    "    arms = args[0]\n",
    "    k = args[1]\n",
    "    method = args[2]\n",
    "    reward_noise = args[3]\n",
    "    epochs = args[4]\n",
    "    MAB_method = MAB(arms,k,method)\n",
    "    return test_model_reward(MAB_method,reward_noise,epochs)\n",
    "\n",
    "    # q.put(1)\n",
    "\n",
    "    # conn.send(test_model_reward(MAB_method,reward_noise,epochs))\n",
    "    # conn.send(1)\n",
    "    # conn.close()\n",
    "\n",
    "def test_plot_reward(arms_range,reward_noise_range,epochs,k_range,stat_sample):\n",
    "    for arms in arms_range:\n",
    "        for reward_noise in reward_noise_range:\n",
    "            for k in k_range:\n",
    "                for method in ['max_expectation','epsilon_greedy1','epsilon_greedy2','epsilon_greedy3','random','ucb','ucb2','optimal']:\n",
    "                    reward_method = np.zeros((stat_sample,epochs))\n",
    "\n",
    "                    parallel_args = []\n",
    "                    for j in range(stat_sample):\n",
    "                        parallel_args.append([arms,k,method,reward_noise,epochs])\n",
    "\n",
    "                    reward_method_wrapper = list(map(test_model_reward_wrapper, iter(parallel_args)))\n",
    "\n",
    "                    with Pool(5) as p:\n",
    "                        print(p.map(test_model_reward_wrapper, iter(parallel_args)))\n",
    "\n",
    "                    # map(test_model_reward_wrapper, (arms,k,method,)))\n",
    "\n",
    "                    # print(1)\n",
    "\n",
    "\n",
    "                    # q = SimpleQueue()\n",
    "                    \n",
    "                    # parallel_processes = []\n",
    "                    # for j in range(stat_sample):\n",
    "                        \n",
    "                    #     parent_conn, child_conn = Pipe()\n",
    "                    #     p = Process(target=test_model_reward_wrapper, args=(child_conn,arms,k,method,))\n",
    "                    #     p.start()\n",
    "                    #     parallel_processes.append([parent_conn,p])\n",
    "\n",
    "                    # with Pool(1) as p:\n",
    "                    #     reward_method_wrapper = p.map(test_model_reward_wrapper, parallel_args)\n",
    "                    # print(reward_method_wrapper)\n",
    "\n",
    "                    # print(3)\n",
    "\n",
    "                    # reward_method_wrapper = []\n",
    "                    # for j in range(stat_sample):\n",
    "                    #     parent_conn = parallel_processes[j][0]\n",
    "                    #     reward_method_wrapper.append(parent_conn.recv())\n",
    "\n",
    "                    # print(4)\n",
    "                    \n",
    "                    # for j in range(stat_sample):\n",
    "                    #     p = parallel_processes[j][1]\n",
    "                    #     p.join()\n",
    "\n",
    "                    # print(2)\n",
    "                    \n",
    "                    for j in range(stat_sample):\n",
    "                        reward_method[j,:] = reward_method_wrapper[j]\n",
    "                    \n",
    "                    # for j in range(stat_sample):\n",
    "                    #     MAB_method = MAB(arms,k,None,method)\n",
    "\n",
    "                    #     reward_method[j,:] = test_model_reward(MAB_method,reward_noise,epochs)\n",
    "\n",
    "                    reward_method_avg = np.mean(reward_method, axis=0)\n",
    "                    reward_method_std = np.std(reward_method, axis=0)\n",
    "\n",
    "                    plt.errorbar(range(epochs),reward_method_avg,yerr=reward_method_std/5.)\n",
    "                \n",
    "                plt.title('Average Cumulative Reward of Top '+str(k)+'-Ranking of '+str(arms)+' arms, noise:'+str(reward_noise))\n",
    "                plt.legend(['max_expectation'.replace('_',' '),\n",
    "                            'epsilon_greedy 0.1'.replace('_',' '),\n",
    "                            'epsilon_greedy 0.5'.replace('_',' '),\n",
    "                            'epsilon_greedy 0.9'.replace('_',' '),\n",
    "                            'random','ucb','ucb2','optimal'])\n",
    "                plt.savefig('reward,k='+str(k)+',arms='+str(arms)+',noise='+str(reward_noise)+'.png')\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arms_range = [10]\n",
    "reward_noise_range = [1/2]\n",
    "epochs = 100\n",
    "k_range_max = 6\n",
    "k_range = range(1,k_range_max+1)\n",
    "stat_sample = 50\n",
    "test_plot_reward(arms_range,reward_noise_range,epochs,k_range,stat_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms_range = [6]\n",
    "reward_noise_range = [1/2]\n",
    "epochs = 100\n",
    "k_range_max = 6\n",
    "k_range = range(1,k_range_max+1)\n",
    "stat_sample = 50\n",
    "test_plot_reward(arms_range,reward_noise_range,epochs,k_range,stat_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arms = 6\n",
    "# reward_noise = 1/2\n",
    "# epochs = 100\n",
    "# k_range = 6\n",
    "# stat_sample = 50\n",
    "# for k in range(1,k_range+1):\n",
    "#     for method in ['max_expectation','epsilon_greedy1','epsilon_greedy2','epsilon_greedy3','random','ucb','ucb2','optimal']:\n",
    "#         reward_index = np.zeros((stat_sample,epochs))\n",
    "#         reward_method = np.zeros((stat_sample,epochs))\n",
    "#         reward_optimal = np.zeros((stat_sample,epochs))\n",
    "#         for j in range(stat_sample):\n",
    "#             MAB_method = MAB(arms,k,None,method)\n",
    "#             MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "\n",
    "#             reward_index[j,:] = range(epochs)\n",
    "#             reward_method[j,:] = test_model_reward(MAB_method,reward_noise,epochs)\n",
    "#             reward_optimal[j,:] = test_model_reward(MAB_optimal,reward_noise,epochs)\n",
    "\n",
    "#         reward_method_avg = np.mean(reward_method, axis=0)\n",
    "#         reward_method_std = np.std(reward_method, axis=0)\n",
    "\n",
    "#         plt.errorbar(range(epochs),reward_method_avg,yerr=reward_method_std/5.)\n",
    "    \n",
    "#     plt.title('Average Cumulative Reward of Top '+str(k)+'-Ranking of '+str(arms)+' arms, noise:'+str(reward_noise))\n",
    "#     plt.legend(['max_expectation'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.1'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.5'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.9'.replace('_',' '),\n",
    "#                 'random','ucb','ucb2','optimal'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms_range = [20]\n",
    "reward_noise_range = [1/2]\n",
    "epochs = 100\n",
    "k_range_max = 6\n",
    "k_range = range(1,k_range_max+1)\n",
    "stat_sample = 50\n",
    "test_plot_reward(arms_range,reward_noise_range,epochs,k_range,stat_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arms = 20\n",
    "# reward_noise = 1/2\n",
    "# epochs = 100\n",
    "# k_range = 6\n",
    "# stat_sample = 50\n",
    "# for k in range(1,k_range+1):\n",
    "#     for method in ['max_expectation','epsilon_greedy1','epsilon_greedy2','epsilon_greedy3','random','ucb','ucb2','optimal']:\n",
    "#         reward_index = np.zeros((stat_sample,epochs))\n",
    "#         reward_method = np.zeros((stat_sample,epochs))\n",
    "#         reward_optimal = np.zeros((stat_sample,epochs))\n",
    "#         for j in range(stat_sample):\n",
    "#             MAB_method = MAB(arms,k,None,method)\n",
    "#             MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "\n",
    "#             reward_index[j,:] = range(epochs)\n",
    "#             reward_method[j,:] = test_model_reward(MAB_method,reward_noise,epochs)\n",
    "#             reward_optimal[j,:] = test_model_reward(MAB_optimal,reward_noise,epochs)\n",
    "\n",
    "#         reward_method_avg = np.mean(reward_method, axis=0)\n",
    "#         reward_method_std = np.std(reward_method, axis=0)\n",
    "\n",
    "#         plt.errorbar(range(epochs),reward_method_avg,yerr=reward_method_std/5.)\n",
    "    \n",
    "#     plt.title('Average Cumulative Reward of Top '+str(k)+'-Ranking of '+str(arms)+' arms, noise:'+str(reward_noise))\n",
    "#     plt.legend(['max_expectation'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.1'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.5'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.9'.replace('_',' '),\n",
    "#                 'random','ucb','ucb2','optimal'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms_range = [6, 10,15,25,50,100]\n",
    "reward_noise_range = [1/2]\n",
    "epochs = 100\n",
    "k_range = [3]\n",
    "stat_sample = 50\n",
    "test_plot_reward(arms_range,reward_noise_range,epochs,k_range,stat_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reward_noise = 1/2\n",
    "# epochs = 100\n",
    "# k = 3\n",
    "# stat_sample = 50\n",
    "# for arms in [6, 10,25,50,100]:\n",
    "#     for method in ['max_expectation','epsilon_greedy1','epsilon_greedy2','epsilon_greedy3','random','ucb','ucb2','optimal']:\n",
    "#         reward_index = np.zeros((stat_sample,epochs))\n",
    "#         reward_method = np.zeros((stat_sample,epochs))\n",
    "#         reward_optimal = np.zeros((stat_sample,epochs))\n",
    "#         for j in range(stat_sample):\n",
    "#             MAB_method = MAB(arms,k,None,method)\n",
    "#             MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "\n",
    "#             reward_index[j,:] = range(epochs)\n",
    "#             reward_method[j,:] = test_model_reward(MAB_method,reward_noise,epochs)\n",
    "#             reward_optimal[j,:] = test_model_reward(MAB_optimal,reward_noise,epochs)\n",
    "\n",
    "#         reward_method_avg = np.mean(reward_method, axis=0)\n",
    "#         reward_method_std = np.std(reward_method, axis=0)\n",
    "\n",
    "#         plt.errorbar(range(epochs),reward_method_avg,yerr=reward_method_std/5.)\n",
    "    \n",
    "#     plt.title('Average Cumulative Reward of Top '+str(k)+'-Ranking of '+str(arms)+' arms, noise:'+str(reward_noise))\n",
    "#     plt.legend(['max_expectation'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.1'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.5'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.9'.replace('_',' '),\n",
    "#                 'random','ucb','ucb2','optimal'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arms_range = [10]\n",
    "reward_noise_range = [.01, .1, .5, 1, 10, 100]\n",
    "epochs = 100\n",
    "k_range = [3]\n",
    "stat_sample = 50\n",
    "test_plot_reward(arms_range,reward_noise_range,epochs,k_range,stat_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arms = 10\n",
    "# reward_noise = 1/2\n",
    "# epochs = 100\n",
    "# k = 3\n",
    "# stat_sample = 50\n",
    "# for reward_noise in [.01, .1, .5, 1, 10, 100]:\n",
    "#     for method in ['max_expectation','epsilon_greedy1','epsilon_greedy2','epsilon_greedy3','random','ucb','ucb2','optimal']:\n",
    "#         reward_index = np.zeros((stat_sample,epochs))\n",
    "#         reward_method = np.zeros((stat_sample,epochs))\n",
    "#         reward_optimal = np.zeros((stat_sample,epochs))\n",
    "#         for j in range(stat_sample):\n",
    "#             MAB_method = MAB(arms,k,None,method)\n",
    "#             MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "\n",
    "#             reward_index[j,:] = range(epochs)\n",
    "#             reward_method[j,:] = test_model_reward(MAB_method,reward_noise,epochs)\n",
    "#             reward_optimal[j,:] = test_model_reward(MAB_optimal,reward_noise,epochs)\n",
    "\n",
    "#         reward_method_avg = np.mean(reward_method, axis=0)\n",
    "#         reward_method_std = np.std(reward_method, axis=0)\n",
    "\n",
    "#         plt.errorbar(range(epochs),reward_method_avg,yerr=reward_method_std/5.)\n",
    "    \n",
    "#     plt.title('Average Cumulative Reward of Top '+str(k)+'-Ranking of '+str(arms)+' arms, noise:'+str(reward_noise))\n",
    "#     plt.legend(['max_expectation'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.1'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.5'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.9'.replace('_',' '),\n",
    "#                 'random','ucb','ucb2','optimal'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arms = 10\n",
    "# reward_noise = 1/1\n",
    "# epochs = 50\n",
    "# k_range = 5\n",
    "# stat_sample = 5\n",
    "# for method in ['max_expectation','epsilon_greedy','random','ucb','ucb2']:\n",
    "#     for k in range(1,k_range):\n",
    "#         regret_index = np.zeros((stat_sample,epochs))\n",
    "#         regret_method = np.zeros((stat_sample,epochs))\n",
    "#         regret_optimal = np.zeros((stat_sample,epochs))\n",
    "#         for j in range(stat_sample):\n",
    "#             MAB_method = MAB(arms,k,None,method)\n",
    "#             MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "\n",
    "#             regret_index[j,:] = range(epochs)\n",
    "#             regret_method[j,:] = test_model(MAB_method,reward_noise,epochs)\n",
    "#             regret_optimal[j,:] = test_model(MAB_optimal,reward_noise,epochs)\n",
    "\n",
    "#         plt.plot(np.transpose(regret_index),np.transpose(regret_method),\n",
    "#                     np.transpose(regret_index),np.transpose(regret_optimal))\n",
    "\n",
    "#         plt.title('Regret of Top '+str(k)+'-Ranking: '+method.replace('_',' '))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# arms = 10\n",
    "# reward_noise = 1/2\n",
    "# epochs = 100\n",
    "# k_range = 5\n",
    "# stat_sample = 50\n",
    "# for k in range(1,k_range):\n",
    "#     for method in ['max_expectation','epsilon_greedy1','epsilon_greedy2','epsilon_greedy3','random','ucb','ucb2','optimal']:\n",
    "#         regret_index = np.zeros((stat_sample,epochs))\n",
    "#         regret_method = np.zeros((stat_sample,epochs))\n",
    "#         regret_optimal = np.zeros((stat_sample,epochs))\n",
    "#         for j in range(stat_sample):\n",
    "#             MAB_method = MAB(arms,k,None,method)\n",
    "#             MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "\n",
    "#             regret_index[j,:] = range(epochs)\n",
    "#             regret_method[j,:] = test_model(MAB_method,reward_noise,epochs)\n",
    "#             regret_optimal[j,:] = test_model(MAB_optimal,reward_noise,epochs)\n",
    "\n",
    "#         regret_method_avg = np.mean(regret_method, axis=0)\n",
    "#         regret_method_std = np.std(regret_method, axis=0)\n",
    "\n",
    "#         plt.errorbar(range(epochs),regret_method_avg,yerr=regret_method_std/5.)\n",
    "    \n",
    "#     plt.title('Regret of Top '+str(k)+'-Ranking')\n",
    "#     plt.legend(['max_expectation'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.1'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.5'.replace('_',' '),\n",
    "#                 'epsilon_greedy 0.9'.replace('_',' '),\n",
    "#                 'random','ucb','ucb2','optimal'])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# arms = 10\n",
    "# reward_noise = 1/1\n",
    "# epochs = 50\n",
    "# k_range = 5\n",
    "# for k in range(1,k_range):\n",
    "#     MAB_max_expectation = MAB(arms,k,None,'max_expectation')\n",
    "#     MAB_epsilon_greedy = MAB(arms,k,None,'epsilon_greedy')\n",
    "#     MAB_random = MAB(arms,k,None,'random')\n",
    "#     MAB_optimal = MAB(arms,k,None,'optimal')\n",
    "#     MAB_ucb = MAB(arms,k,None,'ucb')\n",
    "#     MAB_ucb2 = MAB(arms,k,None,'ucb2')\n",
    "\n",
    "#     regret_max_expectation = test_model(MAB_max_expectation,reward_noise,epochs)\n",
    "#     regret_epsilon_greedy = test_model(MAB_epsilon_greedy,reward_noise,epochs)\n",
    "#     regret_random = test_model(MAB_random,reward_noise,epochs)\n",
    "#     regret_optimal = test_model(MAB_optimal,reward_noise,epochs)\n",
    "#     regret_ucb = test_model(MAB_ucb,reward_noise,epochs)\n",
    "#     regret_ucb2 = test_model(MAB_ucb2,reward_noise,epochs)\n",
    "\n",
    "#     plt.plot(range(len(regret_max_expectation)),regret_max_expectation,\n",
    "#              range(len(regret_epsilon_greedy)),regret_epsilon_greedy,\n",
    "#              range(len(regret_random)),regret_random,\n",
    "#              range(len(regret_optimal)),regret_optimal,\n",
    "#              range(len(regret_ucb)),regret_ucb,\n",
    "#              range(len(regret_ucb2)),regret_ucb2)\n",
    "#     plt.legend(['max expectation','epsilon greedy','random','optimal','ucb','ucb2'])\n",
    "#     plt.title('Regret of Top '+str(k)+'-Ranking')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}